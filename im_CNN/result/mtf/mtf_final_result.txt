Start Time : Fr 1. Sep 10:05:18 CEST 2023
Job ID: 1153729
Node: hades
Partition: gpu_normal_stud
CPU cores: 16
Memory: 71680
GPUs: 0
Python environment: khoa                  *  /home/phamanh/anaconda3/envs/khoa
Current work directory: /home/phamanh/Schreibtisch/bachelorarbeit/im_CNN

device: cuda


Study statistics: 
  Number of finished trials:  100
  Number of pruned trials:  37
  Number of complete trials:  63
  Value:  0.13677294710849194
  Params: 
    batch_size: 32
    drop: 0.24617386581858958
    epochs: 100
    lr: 0.055716328638151014
    n_bins: 5
    optimizer: SGD
    strategy: quantile
    weight_decay: 1.730929873748007e-05
final test best model
https://app.neptune.ai/ba-final/mtf-1/e/MTF1-524
/home/phamanh/Schreibtisch/bachelorarbeit/im_CNN/mtf_cv.py:732: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).
        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)
        for dictionaries or collections that contain unsupported values.
        For more, see https://docs.neptune.ai/help/value_of_unsupported_type
  run["fix_parameters"] = fix_parameters
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
EfficientNet                                            --
├─Sequential: 1-1                                       --
│    └─Conv2dNormActivation: 2-1                        --
│    │    └─Conv2d: 3-1                                 576
│    │    └─BatchNorm2d: 3-2                            64
│    │    └─SiLU: 3-3                                   --
│    └─Sequential: 2-2                                  --
│    │    └─MBConv: 3-4                                 1,448
│    └─Sequential: 2-3                                  --
│    │    └─MBConv: 3-5                                 6,004
│    │    └─MBConv: 3-6                                 10,710
│    └─Sequential: 2-4                                  --
│    │    └─MBConv: 3-7                                 15,350
│    │    └─MBConv: 3-8                                 31,290
│    └─Sequential: 2-5                                  --
│    │    └─MBConv: 3-9                                 37,130
│    │    └─MBConv: 3-10                                102,900
│    │    └─MBConv: 3-11                                102,900
│    └─Sequential: 2-6                                  --
│    │    └─MBConv: 3-12                                126,004
│    │    └─MBConv: 3-13                                208,572
│    │    └─MBConv: 3-14                                208,572
│    └─Sequential: 2-7                                  --
│    │    └─MBConv: 3-15                                262,492
│    │    └─MBConv: 3-16                                587,952
│    │    └─MBConv: 3-17                                587,952
│    │    └─MBConv: 3-18                                587,952
│    └─Sequential: 2-8                                  --
│    │    └─MBConv: 3-19                                717,232
│    └─Conv2dNormActivation: 2-9                        --
│    │    └─Conv2d: 3-20                                409,600
│    │    └─BatchNorm2d: 3-21                           2,560
│    │    └─SiLU: 3-22                                  --
├─AdaptiveAvgPool2d: 1-2                                --
├─Sequential: 1-3                                       --
│    └─Dropout: 2-10                                    --
│    └─Linear: 2-11                                     1,281
================================================================================
Total params: 4,008,541
Trainable params: 4,008,541
Non-trainable params: 0
================================================================================
Warning: string series 'monitoring/e2fa8396/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.
Warning: string 'structure' value was longer than 16384 characters and was truncated. This warning is printed only once.
epoch 0, loss train 0.39, loss test 0.42, score train 0.23, score test 0.23, score final 0.00
epoch 1, loss train 0.28, loss test 0.38, score train 0.26, score test 0.17, score final 0.00
epoch 2, loss train 0.23, loss test 0.32, score train 0.28, score test 0.21, score final 0.00
epoch 3, loss train 0.20, loss test 0.28, score train 0.29, score test 0.22, score final 0.02
epoch 4, loss train 0.18, loss test 0.33, score train 0.30, score test 0.19, score final 0.00
epoch 5, loss train 0.17, loss test 0.26, score train 0.31, score test 0.23, score final 0.00
epoch 6, loss train 0.16, loss test 0.30, score train 0.32, score test 0.21, score final 0.00
epoch 7, loss train 0.15, loss test 0.28, score train 0.33, score test 0.22, score final 0.00
epoch 8, loss train 0.15, loss test 0.30, score train 0.33, score test 0.21, score final 0.00
epoch 9, loss train 0.15, loss test 0.36, score train 0.34, score test 0.16, score final 0.00
epoch 10, loss train 0.14, loss test 0.29, score train 0.34, score test 0.22, score final 0.00
epoch 11, loss train 0.14, loss test 0.30, score train 0.35, score test 0.20, score final 0.16
epoch 12, loss train 0.14, loss test 0.30, score train 0.35, score test 0.21, score final 0.00
epoch 13, loss train 0.13, loss test 0.40, score train 0.35, score test 0.14, score final 0.00
epoch 14, loss train 0.13, loss test 0.33, score train 0.36, score test 0.19, score final 0.00
epoch 15, loss train 0.12, loss test 0.36, score train 0.36, score test 0.17, score final 0.00
epoch 16, loss train 0.12, loss test 0.30, score train 0.37, score test 0.21, score final 0.03
epoch 17, loss train 0.12, loss test 0.26, score train 0.37, score test 0.23, score final 0.03
epoch 18, loss train 0.12, loss test 0.28, score train 0.37, score test 0.22, score final 0.16
epoch 19, loss train 0.11, loss test 0.30, score train 0.38, score test 0.23, score final 0.01
epoch 20, loss train 0.11, loss test 0.32, score train 0.38, score test 0.20, score final 0.06
epoch 21, loss train 0.11, loss test 0.28, score train 0.38, score test 0.22, score final 0.10
epoch 22, loss train 0.11, loss test 0.32, score train 0.39, score test 0.21, score final 0.10
epoch 23, loss train 0.11, loss test 0.27, score train 0.39, score test 0.22, score final 0.07
epoch 24, loss train 0.10, loss test 0.32, score train 0.40, score test 0.19, score final 0.01
epoch 25, loss train 0.10, loss test 0.29, score train 0.40, score test 0.21, score final 0.01
epoch 26, loss train 0.10, loss test 0.32, score train 0.40, score test 0.18, score final 0.00
epoch 27, loss train 0.10, loss test 0.29, score train 0.40, score test 0.21, score final 0.04
epoch 28, loss train 0.09, loss test 0.31, score train 0.40, score test 0.20, score final 0.00
epoch 29, loss train 0.09, loss test 0.33, score train 0.41, score test 0.19, score final 0.00
epoch 30, loss train 0.09, loss test 0.31, score train 0.41, score test 0.19, score final 0.06
epoch 31, loss train 0.09, loss test 0.29, score train 0.41, score test 0.22, score final 0.03
epoch 32, loss train 0.08, loss test 0.30, score train 0.42, score test 0.20, score final 0.00
epoch 33, loss train 0.09, loss test 0.33, score train 0.42, score test 0.19, score final 0.00
epoch 34, loss train 0.09, loss test 0.31, score train 0.42, score test 0.19, score final 0.08
epoch 35, loss train 0.09, loss test 0.33, score train 0.42, score test 0.19, score final 0.09
epoch 36, loss train 0.08, loss test 0.29, score train 0.42, score test 0.21, score final 0.00
epoch 37, loss train 0.08, loss test 0.30, score train 0.42, score test 0.21, score final 0.00
epoch 38, loss train 0.08, loss test 0.29, score train 0.43, score test 0.21, score final 0.00
epoch 39, loss train 0.08, loss test 0.31, score train 0.43, score test 0.19, score final 0.01
epoch 40, loss train 0.08, loss test 0.27, score train 0.43, score test 0.22, score final 0.00
epoch 41, loss train 0.08, loss test 0.29, score train 0.43, score test 0.21, score final 0.02
epoch 42, loss train 0.08, loss test 0.30, score train 0.43, score test 0.20, score final 0.07
epoch 43, loss train 0.08, loss test 0.30, score train 0.44, score test 0.21, score final 0.01
epoch 44, loss train 0.08, loss test 0.30, score train 0.44, score test 0.21, score final 0.00
epoch 45, loss train 0.08, loss test 0.33, score train 0.44, score test 0.17, score final 0.00
epoch 46, loss train 0.08, loss test 0.28, score train 0.44, score test 0.23, score final 0.01
epoch 47, loss train 0.07, loss test 0.27, score train 0.44, score test 0.22, score final 0.01
epoch 48, loss train 0.07, loss test 0.33, score train 0.45, score test 0.18, score final 0.00
epoch 49, loss train 0.07, loss test 0.34, score train 0.44, score test 0.17, score final 0.00
epoch 50, loss train 0.07, loss test 0.29, score train 0.45, score test 0.21, score final 0.01
epoch 51, loss train 0.07, loss test 0.29, score train 0.44, score test 0.21, score final 0.08
epoch 52, loss train 0.07, loss test 0.29, score train 0.46, score test 0.20, score final 0.00
epoch 53, loss train 0.07, loss test 0.28, score train 0.44, score test 0.22, score final 0.09
epoch 54, loss train 0.07, loss test 0.28, score train 0.45, score test 0.21, score final 0.00
epoch 55, loss train 0.07, loss test 0.30, score train 0.45, score test 0.20, score final 0.00
epoch 56, loss train 0.07, loss test 0.30, score train 0.45, score test 0.21, score final 0.00
epoch 57, loss train 0.07, loss test 0.28, score train 0.45, score test 0.21, score final 0.00
epoch 58, loss train 0.07, loss test 0.28, score train 0.46, score test 0.22, score final 0.05
epoch 59, loss train 0.07, loss test 0.32, score train 0.45, score test 0.19, score final 0.00
epoch 60, loss train 0.07, loss test 0.29, score train 0.46, score test 0.21, score final 0.09
epoch 61, loss train 0.07, loss test 0.28, score train 0.45, score test 0.22, score final 0.02
epoch 62, loss train 0.07, loss test 0.30, score train 0.45, score test 0.20, score final 0.01
epoch 63, loss train 0.07, loss test 0.29, score train 0.45, score test 0.20, score final 0.10
epoch 64, loss train 0.07, loss test 0.30, score train 0.46, score test 0.20, score final 0.01
epoch 65, loss train 0.07, loss test 0.32, score train 0.46, score test 0.19, score final 0.00
epoch 66, loss train 0.07, loss test 0.30, score train 0.46, score test 0.21, score final 0.00
epoch 67, loss train 0.07, loss test 0.35, score train 0.46, score test 0.16, score final 0.00
epoch 68, loss train 0.07, loss test 0.33, score train 0.45, score test 0.18, score final 0.04
epoch 69, loss train 0.06, loss test 0.28, score train 0.47, score test 0.22, score final 0.07
epoch 70, loss train 0.07, loss test 0.36, score train 0.47, score test 0.15, score final 0.00
epoch 71, loss train 0.07, loss test 0.31, score train 0.46, score test 0.19, score final 0.00
epoch 72, loss train 0.06, loss test 0.29, score train 0.47, score test 0.20, score final 0.08
epoch 73, loss train 0.06, loss test 0.30, score train 0.47, score test 0.20, score final 0.00
epoch 74, loss train 0.06, loss test 0.30, score train 0.47, score test 0.20, score final 0.01
epoch 75, loss train 0.07, loss test 0.30, score train 0.46, score test 0.21, score final 0.00
epoch 76, loss train 0.06, loss test 0.29, score train 0.47, score test 0.21, score final 0.00
epoch 77, loss train 0.06, loss test 0.29, score train 0.47, score test 0.21, score final 0.00
epoch 78, loss train 0.06, loss test 0.30, score train 0.47, score test 0.21, score final 0.08
epoch 79, loss train 0.06, loss test 0.28, score train 0.48, score test 0.22, score final 0.02
epoch 80, loss train 0.06, loss test 0.32, score train 0.47, score test 0.18, score final 0.00
epoch 81, loss train 0.06, loss test 0.30, score train 0.47, score test 0.21, score final 0.07
epoch 82, loss train 0.06, loss test 0.29, score train 0.48, score test 0.21, score final 0.00
epoch 83, loss train 0.06, loss test 0.29, score train 0.47, score test 0.21, score final 0.12
epoch 84, loss train 0.06, loss test 0.27, score train 0.47, score test 0.22, score final 0.07
epoch 85, loss train 0.06, loss test 0.28, score train 0.48, score test 0.21, score final 0.10
epoch 86, loss train 0.06, loss test 0.30, score train 0.49, score test 0.19, score final 0.07
epoch 87, loss train 0.06, loss test 0.30, score train 0.48, score test 0.21, score final 0.02
epoch 88, loss train 0.06, loss test 0.31, score train 0.49, score test 0.20, score final 0.00
epoch 89, loss train 0.06, loss test 0.27, score train 0.47, score test 0.23, score final 0.00
epoch 90, loss train 0.06, loss test 0.31, score train 0.50, score test 0.20, score final 0.00
epoch 91, loss train 0.06, loss test 0.30, score train 0.49, score test 0.20, score final 0.00
epoch 92, loss train 0.05, loss test 0.32, score train 0.50, score test 0.19, score final 0.05
epoch 93, loss train 0.06, loss test 0.34, score train 0.49, score test 0.17, score final 0.06
epoch 94, loss train 0.06, loss test 0.32, score train 0.49, score test 0.18, score final 0.00
epoch 95, loss train 0.06, loss test 0.33, score train 0.49, score test 0.17, score final 0.00
epoch 96, loss train 0.06, loss test 0.28, score train 0.49, score test 0.22, score final 0.05
epoch 97, loss train 0.06, loss test 0.27, score train 0.49, score test 0.22, score final 0.00
epoch 98, loss train 0.06, loss test 0.30, score train 0.49, score test 0.20, score final 0.01
epoch 99, loss train 0.05, loss test 0.29, score train 0.50, score test 0.20, score final 0.01
Shutting down background jobs, please wait a moment...
Done!
Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.
All 14 operations synced, thanks for waiting!
Explore the metadata in the Neptune app:
https://app.neptune.ai/ba-final/mtf-1/e/MTF1-524/metadata
https://app.neptune.ai/ba-final/mtf-1/e/MTF1-525
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
EfficientNet                                            --
├─Sequential: 1-1                                       --
│    └─Conv2dNormActivation: 2-1                        --
│    │    └─Conv2d: 3-1                                 576
│    │    └─BatchNorm2d: 3-2                            64
│    │    └─SiLU: 3-3                                   --
│    └─Sequential: 2-2                                  --
│    │    └─MBConv: 3-4                                 1,448
│    └─Sequential: 2-3                                  --
│    │    └─MBConv: 3-5                                 6,004
│    │    └─MBConv: 3-6                                 10,710
│    └─Sequential: 2-4                                  --
│    │    └─MBConv: 3-7                                 15,350
│    │    └─MBConv: 3-8                                 31,290
│    └─Sequential: 2-5                                  --
│    │    └─MBConv: 3-9                                 37,130
│    │    └─MBConv: 3-10                                102,900
│    │    └─MBConv: 3-11                                102,900
│    └─Sequential: 2-6                                  --
│    │    └─MBConv: 3-12                                126,004
│    │    └─MBConv: 3-13                                208,572
│    │    └─MBConv: 3-14                                208,572
│    └─Sequential: 2-7                                  --
│    │    └─MBConv: 3-15                                262,492
│    │    └─MBConv: 3-16                                587,952
│    │    └─MBConv: 3-17                                587,952
│    │    └─MBConv: 3-18                                587,952
│    └─Sequential: 2-8                                  --
│    │    └─MBConv: 3-19                                717,232
│    └─Conv2dNormActivation: 2-9                        --
│    │    └─Conv2d: 3-20                                409,600
│    │    └─BatchNorm2d: 3-21                           2,560
│    │    └─SiLU: 3-22                                  --
├─AdaptiveAvgPool2d: 1-2                                --
├─Sequential: 1-3                                       --
│    └─Dropout: 2-10                                    --
│    └─Linear: 2-11                                     1,281
================================================================================
Total params: 4,008,541
Trainable params: 4,008,541
Non-trainable params: 0
================================================================================
Warning: string series 'monitoring/e2fa8396/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.
Warning: string 'structure' value was longer than 16384 characters and was truncated. This warning is printed only once.
Tracking result from best trial in Cross Validation
loss test 0.30, score test 0.20
Bearing 13, RUL pred 11141, RUl true 5730
Bearing 14, RUL pred 9989, RUl true 2890
Bearing 15, RUL pred 18284, RUl true 1610
Bearing 16, RUL pred 20582, RUl true 1460
Bearing 17, RUL pred 15062, RUl true 7570
Bearing 23, RUL pred 13233, RUl true 7530
Bearing 24, RUL pred 3299, RUl true 1390
Bearing 25, RUL pred 13131, RUl true 3090
Bearing 26, RUL pred 5191, RUl true 1290
Bearing 27, RUL pred 1839, RUl true 580
Bearing 33, RUL pred 3729, RUl true 820
score final 0.00
loss final 7410.92

Shutting down background jobs, please wait a moment...
Done!
Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.
All 13 operations synced, thanks for waiting!
Explore the metadata in the Neptune app:
https://app.neptune.ai/ba-final/mtf-1/e/MTF1-525/metadata

